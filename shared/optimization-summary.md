# OpenClaw Memory & Token Optimization â€” Summary

> Collaborative review by **Wiki** (Claude/Anthropic) and **Tank** (GPT/OpenAI)
> Source document: `numerika-ai/openclaw-memory-token-optimization` (~24KB)
> Date: 2026-02-08 | Task: T-20260208-05

---

## Document Overview

"OpenClaw â€” Optymalizacja PamiÄ™ci i ZarzÄ…dzania Tokenami" is an architectural analysis proposing a 3-layer optimization stack for OpenClaw-based multi-agent setups:

| Layer | Name | Core Idea |
|-------|------|-----------|
| 1 | Structured Memory Engine | Replace flat files with Knowledge Graph (Neo4j/SQLite) |
| 2 | Context Orchestrator | Session rotation, dynamic loading, token budgeting |
| 3 | Intelligent Token Router | Model routing by complexity (7B â†’ 120B), cost optimization |

Proposed implementation: 4 phases over 8 weeks with metrics-driven validation.

**Top 3 priorities from document:** Dynamic Skill Loading, Session Rotation, Knowledge Graph.

---

## Our Assessment

### âœ… Adopt Now (Phase 1 â€” already implemented or trivial)

**1. Dynamic Skill Loading** (doc Â§4.3)
- Load skills on-demand, not at boot.
- We already have this: `openclaw-memory-optimizer` module 3 (Skill Router) uses heuristic matching.
- Document proposes confidence scores + LLM fallback â€” overkill for now, heuristics suffice.

**2. Handoff over Compaction** (doc Â§3.2 â€” "light" version)
- Instead of relying on OpenClaw's built-in compaction (lossy at worst moment), generate a handoff file at natural breakpoints.
- Implemented: module 4 (Handoff Generator) creates `shared/handoff/latest.md`.
- No need for a separate "Session Orchestrator" service â€” OpenClaw handles sessions natively.

**3. Tiered Memory (HOT/WARM/COLD)** (doc Â§2.4 â€” without graph)
- Classify memory files by **importance Ã— recency** (not just dates): HOT (active/critical), WARM (recent/relevant), COLD (archive).
- Implemented: module 5 (Memory Tier Manager) with flat-file approach.
- Document's Knowledge Graph (Neo4j/SQLite) is overengineered for 2-bot setup.

**4. Context Budget Monitoring** (doc Â§3.1)
- Simple operational rule, no infrastructure needed:
  - **70% context** â†’ âš ï¸ WARNING: do handoff at next natural break
  - **85% context** â†’ ğŸ”„ ROTATION READY: handoff + close thread / new task
- Prevents compaction at worst moments. We saw this in practice (Wiki hit 137k/200k = 68%).

### ğŸ”® Phase 2 â€” Future (when RTX 3090 is ready)

**5. Model Routing / Local Inference** (doc Layer 3)
- Route TRIVIAL/SIMPLE tasks to local small models (7B on RTX 3090), COMPLEX/CRITICAL to cloud (Opus/GPT-4).
- Requires: Ollama setup on Tank's VM, routing heuristics, soft escalation fallback.
- Directly maps to module 3 (Skill Router) â€” extend with model selection.
- **Not urgent** â€” cloud API works fine for current volume.

### âŒ Rejected (overengineered for our setup)

| Proposal | Why Not |
|----------|---------|
| Knowledge Graph (Neo4j/SQLite) | Flat files + tiering work for 2 bots. Graph adds infra complexity with marginal gain. |
| Session Orchestrator as separate service | OpenClaw handles session lifecycle natively. |
| Full 8-week roadmap with 4 phases | Too much ceremony. We iterate: build â†’ test â†’ ship. |
| Multi-agent token pooling | Only 2 agents, independent budgets are fine. |

---

## What We Already Built (2026-02-08)

| Component | Status | Maps to Doc Section |
|-----------|--------|-------------------|
| Lean AGENTS.md (7.8KB â†’ 1.5KB, âˆ’81%) | âœ… Done | Â§4.1 Context reduction |
| On-demand docs (`docs/agent/`) | âœ… Done | Â§4.3 Dynamic loading |
| Skill Router (module 3) | âœ… Done | Â§4.3 + Layer 3 (basic) |
| Handoff Generator (module 4) | âœ… Done | Â§3.2 Session rotation |
| Memory Tiering (module 5) | âœ… Done | Â§2.4 Tiered memory |
| Context Audit (module 1) | âœ… Done | Â§3.1 Budget monitoring |
| Lean Loader (module 2) | âœ… Done | Â§4.1 Boot optimization |
| Bot-to-bot protocol + taskboard | âœ… Done | (not in doc â€” our addition) |

---

## Key Metrics

| Metric | Before | After | Source |
|--------|--------|-------|--------|
| Static boot context | 11.2 KB | 4.8 KB (âˆ’57%) | module 1 audit |
| AGENTS.md | 7.8 KB | 1.5 KB (âˆ’81%) | manual optimization |
| First-turn headroom (200K) | ~197K | ~199K | estimated |
| Skill modules passing | â€” | 5/5 | Tank sanity check |

---

## Next Steps

1. **Run optimization tests A-E** from `shared/MEMORY-OPTIMIZATION-TEST.md`
2. **Context budget alerts** â€” add operational convention (70%/85% thresholds)
3. **Phase 2 planning** â€” when Bartosz greenlights RTX 3090 local model hosting
4. **Periodic review** â€” re-evaluate this summary monthly against actual usage

---

*Generated collaboratively by Wiki & Tank. Push: `numerika-ai/tank-operator/shared/optimization-summary.md`*
